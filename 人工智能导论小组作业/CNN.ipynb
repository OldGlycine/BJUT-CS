{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "import torchvision\n",
    "import gzip\n",
    "import matplotlib\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "device\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义超参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_epochs = 15\n",
    "num_classes = 10\n",
    "batch_size = 100\n",
    "# learning_rate = 0.0005\n",
    "\n",
    "lr = [0.0005]\n",
    "num_epochs = [15]\n",
    "acc = 0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 读取数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = torchvision.datasets.MNIST(root='../../data/',\n",
    "                                           train=True, \n",
    "                                           transform=transforms.ToTensor(),\n",
    "                                           download=True)\n",
    " \n",
    "test_dataset = torchvision.datasets.MNIST(root='../../data/',\n",
    "                                          train=False, \n",
    "                                          transform=transforms.ToTensor())\n",
    " \n",
    "# Data loader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    " \n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义CNN网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1,6,kernel_size=5,padding=2),\n",
    "            nn.Sigmoid(),\n",
    "            nn.AvgPool2d(kernel_size=2,stride=2),\n",
    "            nn.Conv2d(6,16,kernel_size=5), \n",
    "            nn.Sigmoid(),\n",
    "            nn.AvgPool2d(kernel_size=2,stride=2),\n",
    "            nn.Flatten()\n",
    "         )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(400,120),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(120,84),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(84,10)\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        out = self.conv(x)\n",
    "        out = self.fc(out)\n",
    "        return out\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 开始训练+预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15, Step 200/600, Loss: 2.297340\n",
      "Epoch 1/15, Step 400/600, Loss: 1.539344\n",
      "Epoch 1/15, Step 600/600, Loss: 1.007635\n",
      "Epoch 2/15, Step 200/600, Loss: 0.599984\n",
      "Epoch 2/15, Step 400/600, Loss: 0.467955\n",
      "Epoch 2/15, Step 600/600, Loss: 0.376591\n",
      "Epoch 3/15, Step 200/600, Loss: 0.413985\n",
      "Epoch 3/15, Step 400/600, Loss: 0.333720\n",
      "Epoch 3/15, Step 600/600, Loss: 0.400822\n",
      "Epoch 4/15, Step 200/600, Loss: 0.340355\n",
      "Epoch 4/15, Step 400/600, Loss: 0.252661\n",
      "Epoch 4/15, Step 600/600, Loss: 0.250909\n",
      "Epoch 5/15, Step 200/600, Loss: 0.150509\n",
      "Epoch 5/15, Step 400/600, Loss: 0.121516\n",
      "Epoch 5/15, Step 600/600, Loss: 0.176968\n",
      "Epoch 6/15, Step 200/600, Loss: 0.167003\n",
      "Epoch 6/15, Step 400/600, Loss: 0.277418\n",
      "Epoch 6/15, Step 600/600, Loss: 0.165757\n",
      "Epoch 7/15, Step 200/600, Loss: 0.124223\n",
      "Epoch 7/15, Step 400/600, Loss: 0.132113\n",
      "Epoch 7/15, Step 600/600, Loss: 0.136845\n",
      "Epoch 8/15, Step 200/600, Loss: 0.230968\n",
      "Epoch 8/15, Step 400/600, Loss: 0.150584\n",
      "Epoch 8/15, Step 600/600, Loss: 0.098941\n",
      "Epoch 9/15, Step 200/600, Loss: 0.199844\n",
      "Epoch 9/15, Step 400/600, Loss: 0.167941\n",
      "Epoch 9/15, Step 600/600, Loss: 0.156217\n",
      "Epoch 10/15, Step 200/600, Loss: 0.108009\n",
      "Epoch 10/15, Step 400/600, Loss: 0.123243\n",
      "Epoch 10/15, Step 600/600, Loss: 0.072214\n",
      "Epoch 11/15, Step 200/600, Loss: 0.084539\n",
      "Epoch 11/15, Step 400/600, Loss: 0.165748\n",
      "Epoch 11/15, Step 600/600, Loss: 0.074152\n",
      "Epoch 12/15, Step 200/600, Loss: 0.070917\n",
      "Epoch 12/15, Step 400/600, Loss: 0.134064\n",
      "Epoch 12/15, Step 600/600, Loss: 0.077458\n",
      "Epoch 13/15, Step 200/600, Loss: 0.052571\n",
      "Epoch 13/15, Step 400/600, Loss: 0.106828\n",
      "Epoch 13/15, Step 600/600, Loss: 0.094988\n",
      "Epoch 14/15, Step 200/600, Loss: 0.071089\n",
      "Epoch 14/15, Step 400/600, Loss: 0.078052\n",
      "Epoch 14/15, Step 600/600, Loss: 0.049656\n",
      "Epoch 15/15, Step 200/600, Loss: 0.048182\n",
      "Epoch 15/15, Step 400/600, Loss: 0.015759\n",
      "Epoch 15/15, Step 600/600, Loss: 0.136271\n",
      "Accuracy: 97.83%\n",
      "97.83\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "97.83"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "outter = 0\n",
    "inner = 0\n",
    "model = Net(num_classes).to(device)\n",
    "for l_r in lr: \n",
    "    total_step = len(train_loader)\n",
    "    counter = 1\n",
    "    l = []\n",
    "    index = []\n",
    "    for epochs in num_epochs:\n",
    "        # model = Net(num_classes).to(device)\n",
    "        # 定义优化器和loss函数\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=l_r)\n",
    "        for epoch in range(epochs):\n",
    "            for i, (images, labels) in enumerate(train_loader):\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                if (i+1) % 200 == 0:\n",
    "                    print ('Epoch {}/{}, Step {}/{}, Loss: {:.6f}' \n",
    "                        .format(epoch+1, epochs, i+1, total_step, loss.item()))\n",
    "                    l.append(loss.item())\n",
    "                    index.append(counter)\n",
    "                    counter += 1\n",
    "        model.eval() \n",
    "        with torch.no_grad():\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for images, labels in test_loader:\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    " \n",
    "            print('Accuracy: ' + str(100 * correct / total) + '%')\n",
    "        acc = 100 * correct / total\n",
    "        print(acc)\n",
    "        plt.title(\"Lr:\" + str(l_r) + ' ' + \"Epoch:\" + str(epochs) + ' acc:' + str(acc) + '% Unit:' + str(counter))\n",
    "        plt.xlabel('200 steps/Unit')\n",
    "        plt.ylabel('loss')\n",
    "        plt.plot(index, l, linewidth=1, color=\"orange\", marker=\"o\",label=\"Lr:\" + str(l_r) + ' ' + \"Epoch:\" + str(epochs))\n",
    "        name = \"Lr\" + str(l_r) + ' ' + \"Epoch\" + str(epochs) + \".jpg\"\n",
    "        plt.savefig(name)\n",
    "        plt.clf()\n",
    "        inner += 1\n",
    "        counter = 1\n",
    "        l.clear()\n",
    "        index.clear()\n",
    "    counter = 1\n",
    "    inner=0\n",
    "    outter += 1\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 7.2253804  -9.852907    0.1318919  -4.524937   -4.214551   -1.3858266\n",
      "  0.32618657 -5.759858   -0.4751768  -1.0035301 ]\n",
      "识别结果为:0\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i, (images, labels) in enumerate(train_loader):\n",
    "    if i == 1:\n",
    "        break\n",
    "    plt.imshow(images[i][0])\n",
    "    images = images.to(device)\n",
    "    res = model(images).detach().cpu().numpy()\n",
    "    idx = res[i]\n",
    "    print(idx)\n",
    "    print('识别结果为:' + str(np.where(idx == max(idx))[0][0]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
